install.packages("corrplot")
library(corrplot)
install.packages("rsample")
library(rsample)
install.packages("yardstick")
library(yardstick)
install.packages("car")
library(car)
# Read the data into R (adjust file name if needed)
cars_raw <- read_excel("dataset_final.xlsx")
# Step 1: Convert character columns that look numeric (e.g., "1042") into actual numbers
cars_clean <- cars_raw %>%
mutate(across(
where(is.character),
~ ifelse(
grepl("^[0-9.,]+$", .x),
suppressWarnings(as.numeric(gsub(",", "", .x))),
.x
)
)) %>%
# Step 2: Convert energy consumption to numeric and replace NAs with 0
mutate(`Energif√∂rbrukning (Wh/km)` = replace_na(as.numeric(`Energif√∂rbrukning (Wh/km)`), 0))
# Step 1: Convert character columns that look numeric (e.g., "1042") into actual numbers
cars_clean <- cars_raw %>%
mutate(across(
where(is.character),
~ ifelse(
grepl("^[0-9.,]+$", .x),
suppressWarnings(as.numeric(gsub(",", "", .x))),
.x
)
)) %>%
# Step 2: Convert energy consumption to numeric and replace NAs with 0
mutate(`Energif√∂rbrukning (Wh/km)` = replace_na(as.numeric(`Energif√∂rbrukning (Wh/km)`), 0))
glimpse(cars_clean)
# Rule of thumb: a car loses between 15‚Äì20% of its value per year
# Source: https://exchange.aaa.com/automotive/car-buying-and-selling/car-resale-value
# ‚ÄúIf I buy this used car today for 120000 kr, how much will it lose in value each year going forward?‚Äù
# Annual depreciation = price √ó 0.15
cars_clean$"V√§rdeminskning (kr) per √•r" <- round(cars_clean$`Pris (kr)` * 0.15, 0)
cars_clean$"V√§rdeminskning (kr) per √•r" <- round(cars_clean$`Pris (kr)` * 0.15, 0)
# If I pay 20% myself and borrow the rest at 7.45% annual interest, this shows how much I pay in interest cost per year
cars_clean$"L√•n (kr) per √•r" <- round(cars_clean$`Pris (kr)` * 0.80 * 0.0745, 0)
# Set current fuel and electricity prices (kr per liter or kWh)
pris_bensin <- 15.54   # kr/liter
pris_diesel <- 16.59   # kr/liter
pris_el <- 1.50        # kr/kWh
# Create column for fuel cost based on fuel type and consumption
cars_clean$"Drivmedelskostnad (kr) f√∂r 10000 km" <- NA  # Initialize column
# Bensin (Gasoline)
bensin_index <- grepl("Bensin", cars_clean$Br√§nsle, ignore.case = TRUE)
cars_clean$"Drivmedelskostnad (kr) f√∂r 10000 km"[bensin_index] <- round(cars_clean$`Br√§nslef√∂rbrukning (l / 100 km)`[bensin_index] * 100 * pris_bensin, 0)
# Diesel
diesel_index <- grepl("Diesel", cars_clean$Br√§nsle, ignore.case = TRUE)
cars_clean$"Drivmedelskostnad (kr) f√∂r 10000 km"[diesel_index] <- round(cars_clean$`Br√§nslef√∂rbrukning (l / 100 km)`[diesel_index] * 100 * pris_diesel, 0)
# Hybrid or environmentally friendly fuel
hybrid_index <- grepl("Milj√∂br√§nsle|Hybrid", cars_clean$Br√§nsle, ignore.case = TRUE)
cars_clean$"Drivmedelskostnad (kr) f√∂r 10000 km"[hybrid_index] <- round(cars_clean$`Br√§nslef√∂rbrukning (l / 100 km)`[hybrid_index] * 100 * pris_bensin, 0)
# Electric cars (electricity only)
elbilar_index <- !is.na(cars_clean$`Energif√∂rbrukning (Wh/km)`) & is.na(cars_clean$`Br√§nslef√∂rbrukning (l / 100 km)`)
cars_clean$"Drivmedelskostnad (kr) f√∂r 10000 km"[elbilar_index] <- round((cars_clean$`Energif√∂rbrukning (Wh/km)`[elbilar_index] / 1000) * 10000 * pris_el, 0)
# Plug-in hybrids (both electricity and fuel)
laddhybrid_index <- !is.na(cars_clean$`Energif√∂rbrukning (Wh/km)`) & !is.na(cars_clean$`Br√§nslef√∂rbrukning (l / 100 km)`)
# Assume 50% of driving is electric and 50% is fuel-based
el_kostnad <- (cars_clean$`Energif√∂rbrukning (Wh/km)`[laddhybrid_index] / 1000) * 5000 * pris_el
br√§nsle_kostnad <- cars_clean$`Br√§nslef√∂rbrukning (l / 100 km)`[laddhybrid_index] * 50 * pris_bensin
cars_clean$"Drivmedelskostnad (kr) f√∂r 10000 km"[laddhybrid_index] <- round(el_kostnad + br√§nsle_kostnad, 0)
# To avoid multicollinearity in regression (i.e. one variable being a perfect combination of others),
# we use k‚Äì1 dummy variables for k categories
cars_clean <- dummy_cols(
cars_clean,
select_columns = "Br√§nsle",
remove_first_dummy = TRUE,    # referenskategori: El
remove_selected_columns = FALSE
)
cars_clean$Br√§nsle <- factor(cars_clean$Br√§nsle)
cars_clean$Br√§nsle <- relevel(cars_clean$Br√§nsle, ref = "El")
# Replace values in 'Biltyp' by grouping them into broader categories
cars_clean <- cars_clean %>%
mutate(
Biltyp = case_when(
Biltyp %in% c("Halvkombi", "Kombi", "Sedan") ~ "Personbil",
Biltyp %in% c("SUV", "Familjebuss", "Yrkesfordon", "Sk√•pbil") ~ "Storbil",
Biltyp %in% c("Coup√©", "Cab") ~ "Sportbil",
TRUE ~ "Annat"
),
Biltyp = factor(Biltyp)
)
#_____________________________________________________________________________
# 3.Create final model dataset where we use *only* factors
cars_clean_inf <- cars_clean %>%
mutate(
Helf√∂rs√§kring_log = log(`Helf√∂rs√§kring (kr / √•r)`),
Pris_log = log(`Pris (kr)`),
Skatt_log = log(`Fordonsskatt (kr / √•r)` + 1),
M√§tar_log = log(`M√§tarst√§llning (km)` + 1),
CO2_log = log(`Koldioxidutsl√§pp blandad (NEDC) g/km` + 1),
Br√§nslef√∂rb_log = log(`Br√§nslef√∂rbrukning (l / 100 km)` + 1)
) %>%
select(
Helf√∂rs√§kring_log,
H√§stkrafter,
Modell√•r,
M√§tar_log,
CO2_log,
Br√§nslef√∂rb_log,
Br√§nsle,
`Koldioxidutsl√§pp blandad (NEDC) g/km`,
Biltyp,
Pris_log,
Skatt_log
)
glimpse(cars_clean_inf)
cars_clean_inf %>%
select_if(is.numeric) %>%
summarise(across(everything(), ~ cor(.x, cars_clean_inf[["Helf√∂rs√§kring_log"]], use = "complete.obs"))) %>%
pivot_longer(cols = everything(), names_to = "Variable", values_to = "Correlation") %>%
arrange(desc(abs(Correlation)))
model_f <- lm(Helf√∂rs√§kring_log ~ ., data = cars_clean_inf %>%
select(-Br√§nslef√∂rb_log, -Skatt_log, -`Koldioxidutsl√§pp blandad (NEDC) g/km`))
summary(model_f)
confint(model_f) #Confidence interval
#Diagnostic Pplots
par(mfrow = c(2, 2))
plot(model_f)
par(mfrow = c(1, 1))
vif(model_f)
# Inference for pris-model
model_p <- lm(Pris_log ~ ., data = cars_clean_inf %>%
select(-Helf√∂rs√§kring_log, -Skatt_log, -Br√§nslef√∂rb_log, -`Koldioxidutsl√§pp blandad (NEDC) g/km`))
summary(model_p)
confint(model_p)
par(mfrow = c(2, 2))
plot(model_p)
par(mfrow = c(1, 1))
vif(model_p)
# Create a new data
cars_price <- cars_clean %>%
select(
pris                 = `Pris (kr)`,
fordonsskatt         = `Fordonsskatt (kr / √•r)`,
helforsakring        = `Helf√∂rs√§kring (kr / √•r)`,
hastkrafter          = H√§stkrafter,
matarstallning       = `M√§tarst√§llning (km)`
) %>%
mutate(
pris_log = log(pris),
fordonsskatt_log = log(fordonsskatt + 1),
helforsakring_log = log(helforsakring + 1),
hastkrafter_log = log(hastkrafter + 1),
matarstallning_log = log(matarstallning + 1)
) %>%
select(
pris,
ends_with("_log")
)
glimpse(cars_price)
#_________________________________________________________________________
#Building the model
# Split into training and test sets (80/20)
set.seed(123)
split <- initial_split(cars_price, prop = 0.8)
train <- training(split)
test <- testing(split)
# Train the model
model_f <- lm(`pris_log` ~., data = train)
# Train the model
model_f <- lm(`pris_log` ~., data = train)
summary(model_f)
confint(model_f)
#Split to val
val_split <- initial_split(train, prop = 0.75)
train_final <- training(val_split)
val <- testing(val_split)
#Evaluate the model
val$pred <- predict(model_f, newdata = val)
#Calculate RMSE and MAE
rmse <- sqrt(mean((val$pris_log - val$pred)^2))
mae <- mean(abs(val$pris_log - val$pred))
cat("\n Modellprestanda p√• valideringsdata:\n")
cat("üîπ RMSE (Root Mean Squared Error):", round(rmse, 3), "\n")
cat("üîπ MAE  (Mean Absolute Error):    ", round(mae, 3), "\n")
# Predictions on the test data
test$pred <- predict(model_f, newdata = test)
# Evaluate RMSE, MAE och R¬≤
rmse_test <- sqrt(mean((test$pris_log - test$pred)^2))
mae_test  <- mean(abs(test$pris_log - test$pred))
sst <- sum((test$pris_log - mean(test$pris_log))^2)
sse <- sum((test$pris_log - test$pred)^2)
r2_test <- 1 - sse/sst
cat("\n Modellprestanda p√• testdata:\n")
cat("üîπ RMSE (Root Mean Squared Error):", round(rmse_test, 3), "\n")
cat("üîπ MAE  (Mean Absolute Error):    ", round(mae_test, 3), "\n")
cat("üîπ R¬≤   (F√∂rklarad varians):      ", round(r2_test, 3), "\n")
# Predictions against real values in both validation data and test data
val$dataset <- "Val"
test$dataset <- "Test"
combined <- bind_rows(val, test)
# Prediction vs reality
library(ggplot2)
ggplot(combined, aes(x = pris_log, y = pred, color = dataset)) +
geom_point(alpha = 0.6) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
theme_minimal() +
labs(
title = "Predikterat vs Verkligt (log(pris))",
x = "Verkligt log-pris",
y = "Predikterat log-pris",
color = "Dataset"
)
# table with results
resultat_df <- tibble(
Dataset = c("Val", "Test"),
RMSE = c(rmse_val, rmse_test),
MAE  = c(mae, mae_test),
R2   = c(r2_val, r2_test)
)
rmse_val <- sqrt(mean((val$pris_log - val$pred)^2))
mae_val <- mean(abs(val$pris_log - val$pred))
cat("\n Modellprestanda p√• valideringsdata:\n")
cat("üîπ RMSE (Root Mean Squared Error):", round(rmse_val, 3), "\n")
cat("üîπ MAE  (Mean Absolute Error):    ", round(mae_val, 3), "\n")
# Predictions on the test data
test$pred <- predict(model_f, newdata = test)
# Evaluate RMSE, MAE och R¬≤
rmse_test <- sqrt(mean((test$pris_log - test$pred)^2))
mae_test  <- mean(abs(test$pris_log - test$pred))
sst <- sum((test$pris_log - mean(test$pris_log))^2)
sse <- sum((test$pris_log - test$pred)^2)
r2_test <- 1 - sse/sst
cat("\n Modellprestanda p√• testdata:\n")
cat("üîπ RMSE (Root Mean Squared Error):", round(rmse_test, 3), "\n")
cat("üîπ MAE  (Mean Absolute Error):    ", round(mae_test, 3), "\n")
cat("üîπ R¬≤   (F√∂rklarad varians):      ", round(r2_test, 3), "\n")
# Predictions against real values in both validation data and test data
val$dataset <- "Val"
test$dataset <- "Test"
combined <- bind_rows(val, test)
# Prediction vs reality
library(ggplot2)
ggplot(combined, aes(x = pris_log, y = pred, color = dataset)) +
geom_point(alpha = 0.6) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
theme_minimal() +
labs(
title = "Predikterat vs Verkligt (log(pris))",
x = "Verkligt log-pris",
y = "Predikterat log-pris",
color = "Dataset"
)
# table with results
resultat_df <- tibble(
Dataset = c("Val", "Test"),
RMSE = c(rmse_val, rmse_test),
MAE  = c(mae, mae_test),
R2   = c(r2_val, r2_test)
)
resultat_df <- tibble(
Dataset = c("Val", "Test"),
RMSE = c(rmse_val, rmse_test),
MAE  = c(mae, mae_test),
)
print(resultat_df)
#| label: setup
#| message: false
#| warning: false
#| results: hide
packages <- c(
"tidyverse", "readxl", "fastDummies", "corrplot",
"rsample", "GGally", "ggcorrplot", "car", "xgboost", "yardstick", "tibble", "ranger", "glmnet", "vip"
)
idx <- packages %in% rownames(installed.packages())
if (any(!idx)) install.packages(packages[!idx])
# Read the data (adjust file name/path if needed) -------------------------
cars_raw <- read_excel("dataset_final.xlsx")
# 1. Convert character columns that look numeric into numeric --------------
cars_clean <- cars_raw %>%
mutate(across(
where(is.character),
~ ifelse(
grepl("^[0-9.,]+$", .x),
suppressWarnings(as.numeric(gsub(",", "", .x))),
.x
)
)) %>%
# 2. Convert energy consumption to numeric and replace NAs with 0 --------
mutate(`Energif√∂rbrukning (Wh/km)` = replace_na(as.numeric(`Energif√∂rbrukning (Wh/km)`), 0)) %>%
# 3. Convert Modell√•r to factor ------------------------------------------
mutate(Modell√•r = factor(Modell√•r)) %>%
# 4. Remove Biltyp completely --------------------------------------------
select(-Biltyp)
glimpse(cars_clean)
# Visualise distributions of all numeric variables -----------------------
numeric_vars <- cars_clean %>%
select(where(is.numeric)) %>%
select(where(~ n_distinct(.) > 2)) %>%
pivot_longer(everything(), names_to = "Variable", values_to = "Value")
ggplot(numeric_vars, aes(Value)) +
geom_histogram(bins = 30, fill = "steelblue", color = "white") +
facet_wrap(~ Variable, scales = "free") +
theme_minimal() +
labs(x = NULL, y = "Count")
# Final modelling dataset -------------------------------------------------
cars_clean_inf <- cars_clean %>%
mutate(
Helf√∂rs√§kring_log = log(`Helf√∂rs√§kring (kr / √•r)`),
H√§stkrafter_log        = log(H√§stkrafter + 1),
Pris_log          = log(`Pris (kr)`),
Skatt_log         = log(`Fordonsskatt (kr / √•r)` + 1),
M√§tar_log         = log(`M√§tarst√§llning (km)` + 1),
Br√§nslef√∂rb_log   = log(`Br√§nslef√∂rbrukning (l / 100 km)` + 1)
) %>%
select(
Helf√∂rs√§kring_log, H√§stkrafter_log, Modell√•r, M√§tar_log,
Br√§nslef√∂rb_log, Br√§nsle, Pris_log, Skatt_log, `Koldioxidutsl√§pp blandad (NEDC) g/km`
)
glimpse(cars_clean_inf)
cars_clean_inf %>%
select(where(is.numeric)) %>%
summarise(across(everything(), ~ cor(.x, cars_clean_inf$Helf√∂rs√§kring_log, use = "complete.obs"))) %>%
pivot_longer(everything(), names_to = "Variable", values_to = "Correlation") %>%
arrange(desc(abs(Correlation)))
model_f <- lm(Helf√∂rs√§kring_log ~ ., data = cars_clean_inf%>%select(-Br√§nslef√∂rb_log, -Skatt_log, -`Koldioxidutsl√§pp blandad (NEDC) g/km`))
summary(model_f)
confint(model_f)
# Diagnostics -------------------------------------------------------------
par(mfrow = c(2, 2))
plot(model_f)
vif(model_f)
model_p <- lm(Pris_log ~ .,  data = cars_clean_inf %>% select(-Helf√∂rs√§kring_log, -Skatt_log, -Br√§nslef√∂rb_log, -`Koldioxidutsl√§pp blandad (NEDC) g/km`))
summary(model_p)
confint(model_p)
# Diagnostics -------------------------------------------------------------
par(mfrow = c(2, 2))
plot(model_p)
par(mfrow = c(1, 1))
# Diagnostics -------------------------------------------------------------
par(mfrow = c(2, 2))
plot(model_p)
vif(model_p)
model_s <- lm(Skatt_log ~ .,  data = cars_clean_inf %>% select(-Helf√∂rs√§kring_log, -Br√§nslef√∂rb_log))
summary(model_s)
confint(model_s)
# Diagnostics -------------------------------------------------------------
par(mfrow = c(2, 2))
plot(model_s)
vif(model_s)
library(dplyr)
library(fastDummies)
library(janitor)
library(rsample)
library(ranger)
library(purrr)
library(yardstick)
library(tibble)
library(vip)
# Define log-transform target columns
log_vars <- c(
"pris_log",
"fordonsskatt_log",
"helforsakring_log",
"hastkrafter_log",
"matarstallning_log"
)
# Clean and transform the dataset
cars_price <- cars_clean %>%
mutate(id = row_number()) %>%
transmute(
id,
fordonsbenamning = Fordonsben√§mning,
handelsbeteckning = Handelsbeteckning,
pris = `Pris (kr)`,
pris_log = log(pris),
fordonsskatt_log = log(`Fordonsskatt (kr / √•r)` + 1),
helforsakring_log = log(`Helf√∂rs√§kring (kr / √•r)` + 1),
hastkrafter_log = log(H√§stkrafter + 1),
matarstallning_log = log(`M√§tarst√§llning (km)` + 1),
#co2 = `Koldioxidutsl√§pp blandad (NEDC) g/km`
) %>%
clean_names()
View(cars_price)
# -----------------------------------------------------------
# Step 2: Compute outlier thresholds before splitting
# -----------------------------------------------------------
compute_outlier_bounds <- function(df, columns, lower = 0.01, upper = 0.99) {
map(columns, function(col) {
tibble(
variable = col,
q_low = quantile(df[[col]], lower, na.rm = TRUE),
q_high = quantile(df[[col]], upper, na.rm = TRUE)
)
}) %>% bind_rows()
}
apply_outlier_filter <- function(df, bounds) {
for (i in seq_len(nrow(bounds))) {
col <- bounds$variable[i]
df <- df %>% filter(.data[[col]] >= bounds$q_low[i], .data[[col]] <= bounds$q_high[i])
}
df
}
# -----------------------------------------------------------
# Step 3: Initial Split (80/20) stratified by log(price)
# -----------------------------------------------------------
set.seed(123)
split <- initial_split(cars_price, prop = 0.8, strata = pris_log)
train <- training(split)
test  <- testing(split)
# Apply outlier removal
train <- apply_outlier_filter(train, outlier_bounds)
outlier_bounds <- compute_outlier_bounds(cars_price, log_vars)
# -----------------------------------------------------------
# Step 3: Initial Split (80/20) stratified by log(price)
# -----------------------------------------------------------
set.seed(123)
split <- initial_split(cars_price, prop = 0.8, strata = pris_log)
train <- training(split)
test  <- testing(split)
# Apply outlier removal
train <- apply_outlier_filter(train, outlier_bounds)
test  <- apply_outlier_filter(test, outlier_bounds)
# -----------------------------------------------------------
# Step 4: Hyperparameter Grid Search via 5-fold Cross-validation
# -----------------------------------------------------------
set.seed(123)
folds <- vfold_cv(train, v = 5, strata = pris_log)
param_grid <- expand.grid(
num.trees     = c(50, 200),
mtry          = floor(ncol(train)/2),
min.node.size = c(1, 10),
max.depth     = c(2, 12)
)
cv_results <- purrr::map_dfr(seq_len(nrow(param_grid)), function(i) {
p <- param_grid[i, ]
fold_metrics <- map_dbl(folds$splits, function(spl) {
tr <- analysis(spl)
va <- assessment(spl)
mod <- ranger(
pris_log ~ .,
data          = tr %>% select(-id, -fordonsbenamning, -handelsbeteckning, -pris),
num.trees     = p$num.trees,
mtry          = p$mtry,
min.node.size = p$min.node.size,
max.depth     = p$max.depth
)
pred <- predict(mod, va)$predictions
rmse_vec(truth = va$pris_log, estimate = pred)
})
tibble(
num.trees     = p$num.trees,
mtry          = p$mtry,
min.node.size = p$min.node.size,
max.depth     = p$max.depth,
RMSE_log_CV   = mean(fold_metrics)
)
})
# Select best hyperparameters based on CV
best_params <- cv_results %>%
slice_min(RMSE_log_CV, n = 1)
# -----------------------------------------------------------
# Step 5: Train final model on entire training set
# -----------------------------------------------------------
final_model <- ranger(
pris_log ~ .,
data          = train %>% select(-id, -fordonsbenamning, -handelsbeteckning, -pris),
num.trees     = best_params$num.trees,
mtry          = best_params$mtry,
min.node.size = best_params$min.node.size,
max.depth     = best_params$max.depth,
importance    = "impurity"
)
# -----------------------------------------------------------
# Step 7: Evaluate RMSE (final hold-out test set)
# -----------------------------------------------------------
# Predictions on test set (hold-out)
test_pred_log <- predict(final_model, test)$predictions
test_pred_sek <- exp(test_pred_log)
# Calculate RMSE for test set
rmse_test_log <- rmse_vec(test$pris_log, test_pred_log)
rmse_test_sek <- rmse_vec(test$pris, test_pred_sek)
# Compute SEK equivalent of the CV RMSE_log (0.228)
# Using mean predicted price (SEK) from test set as scaling factor
mean_test_pred_sek <- mean(test_pred_sek)
rmse_cv_sek_equiv <- mean_test_pred_sek * best_params$RMSE_log_CV
# Step 8: Results (CV vs. Test)
results_summary <- tibble(
Dataset  = c("5-fold CV (grid-search)", "Test set (hold-out)"),
RMSE_log = round(c(best_params$RMSE_log_CV, rmse_test_log), 4),
RMSE_SEK = round(c(rmse_cv_sek_equiv, rmse_test_sek), 0)
)
print(results_summary)
View(param_grid)
# -----------------------------------------------------------
# Step 9: Residual Analysis ‚Äì overpriced vs. underpriced cars
# -----------------------------------------------------------
test_results <- test %>%
mutate(
pred_log = test_pred_log,
pred_sek = test_pred_sek,
diff_sek = pris - pred_sek,
diff_pct = 100 * diff_sek / pred_sek
) %>%
select(id, fordonsbenamning, handelsbeteckning, pris, pred_sek, diff_sek, diff_pct)
overpriced  <- test_results %>% arrange(desc(diff_sek)) %>% slice_head(n = 5)
underpriced <- test_results %>% arrange(diff_sek) %>% slice_head(n = 5)
cat("\nTop 5 Overpriced Cars:\n")
print(overpriced)
cat("\nTop 5 Underpriced Cars:\n")
print(underpriced)
# -----------------------------------------------------------
# Step 10: Variable importance visualization
# -----------------------------------------------------------
vip(final_model, num_features = 10, bar = TRUE)
